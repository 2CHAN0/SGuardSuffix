{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SGuard Suffix Attack Training\n",
                "\n",
                "This notebook runs the SGuard Suffix Attack training and saves results to Google Drive."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Mount Google Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Setup Environment\n",
                "Clone the repository and install dependencies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone the repository\n",
                "# TODO: Replace with your actual repository URL\n",
                "!rm -rf /content/sguard_attack\n",
                "!git clone https://github.com/2CHAN0/SGuardSuffix.git sguard_attack\n",
                "\n",
                "# Install dependencies\n",
                "!pip install -r sguard_attack/requirements.txt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Caching (Google Drive)\n",
                "Check if the model exists in Google Drive. If not, download and save it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "import sys\n",
                "\n",
                "# Add the package to path\n",
                "sys.path.append('/content')\n",
                "from sguard_attack.config import Config\n",
                "\n",
                "# Define Google Drive Model Path\n",
                "# Using the project name as the base folder\n",
                "drive_model_dir = \"/content/drive/MyDrive/SGuardSuffix/models/SamsungSDS-Research/SGuard-JailbreakFilter-2B-v1\"\n",
                "\n",
                "if os.path.exists(drive_model_dir) and os.listdir(drive_model_dir):\n",
                "    print(f\"Model found in Google Drive: {drive_model_dir}\")\n",
                "    model_path = drive_model_dir\n",
                "else:\n",
                "    print(f\"Model not found in Drive. Downloading from Hugging Face: {Config.MODEL_ID}\")\n",
                "    \n",
                "    # Download and Save\n",
                "    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_ID)\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        Config.MODEL_ID,\n",
                "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
                "        trust_remote_code=True\n",
                "    )\n",
                "    \n",
                "    print(f\"Saving model to Google Drive: {drive_model_dir}\")\n",
                "    tokenizer.save_pretrained(drive_model_dir)\n",
                "    model.save_pretrained(drive_model_dir)\n",
                "    \n",
                "    model_path = drive_model_dir\n",
                "    print(\"Model saved successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Run Training\n",
                "Initialize the model and run the attack, saving checkpoints to Google Drive."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import datetime\n",
                "import torch\n",
                "\n",
                "from sguard_attack.config import Config\n",
                "from sguard_attack.model_utils import load_model_and_tokenizer\n",
                "from sguard_attack.dataset import Dataset\n",
                "from sguard_attack.attack import GCGAttack\n",
                "\n",
                "# Setup Save Directory\n",
                "# Create a timestamped directory in Google Drive\n",
                "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "save_base_dir = f\"/content/drive/MyDrive/SGuard_Training_Results/{timestamp}\"\n",
                "os.makedirs(save_base_dir, exist_ok=True)\n",
                "print(f\"Saving results to: {save_base_dir}\")\n",
                "\n",
                "# Load Model & Tokenizer from the cached path\n",
                "model, tokenizer = load_model_and_tokenizer(model_name_or_path=model_path)\n",
                "\n",
                "# Load Dataset\n",
                "dataset = Dataset()\n",
                "\n",
                "# Initialize Attack\n",
                "attacker = GCGAttack(model, tokenizer)\n",
                "\n",
                "print(f\"Starting attack on {len(dataset)} prompts...\")\n",
                "\n",
                "results = []\n",
                "\n",
                "for i, malicious_prompt in enumerate(dataset):\n",
                "    print(f\"\\n[{i+1}/{len(dataset)}] Attacking prompt: {malicious_prompt}\")\n",
                "    \n",
                "    # Run attack with saving enabled\n",
                "    # save_interval=10 means it will save a checkpoint every 10 steps\n",
                "    best_suffix, best_suffix_ids, best_loss = attacker.run(\n",
                "        malicious_prompt, \n",
                "        save_dir=save_base_dir, \n",
                "        save_interval=10\n",
                "    )\n",
                "    \n",
                "    print(f\"Result for '{malicious_prompt}':\")\n",
                "    print(f\"  Best Suffix: {best_suffix}\")\n",
                "    print(f\"  Final Loss: {best_loss:.4f}\")\n",
                "    \n",
                "    # Verification (String)\n",
                "    full_input = malicious_prompt + \" \" + best_suffix\n",
                "    inputs = tokenizer(full_input, return_tensors=\"pt\").to(Config.DEVICE)\n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
                "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "        \n",
                "    print(f\"  Model Output (String): {response}\")\n",
                "\n",
                "    # Verification (Tensor)\n",
                "    prompt_ids = tokenizer(malicious_prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(Config.DEVICE)\n",
                "    if best_suffix_ids.dim() == 1:\n",
                "        best_suffix_ids = best_suffix_ids.unsqueeze(0)\n",
                "    \n",
                "    full_input_ids = torch.cat([prompt_ids, best_suffix_ids], dim=1)\n",
                "    with torch.no_grad():\n",
                "        outputs_tensor = model.generate(full_input_ids, max_new_tokens=10)\n",
                "        new_tokens = outputs_tensor[0][full_input_ids.shape[1]:]\n",
                "        response_tensor = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
                "    \n",
                "    print(f\"  Model Output (Tensor): {response_tensor}\")\n",
                "    \n",
                "    results.append({\n",
                "        \"prompt\": malicious_prompt,\n",
                "        \"suffix\": best_suffix,\n",
                "        \"loss\": best_loss,\n",
                "        \"output_string\": response,\n",
                "        \"output_tensor\": response_tensor\n",
                "    })\n",
                "\n",
                "# Save Final Summary\n",
                "import json\n",
                "summary_path = os.path.join(save_base_dir, \"final_summary.json\")\n",
                "with open(summary_path, 'w') as f:\n",
                "    json.dump(results, f, indent=4)\n",
                "print(f\"\\nFinal summary saved to {summary_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Official Inference Verification\n",
                "Verify the attack results using the official SGuard inference logic."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "\n",
                "# Constants\n",
                "SAFE_TOKEN = 'safe'\n",
                "UNSAFE_TOKEN = 'unsafe'\n",
                "\n",
                "# We reuse the already loaded model and tokenizer\n",
                "# model, tokenizer = load_model_and_tokenizer(model_name_or_path=model_path)\n",
                "\n",
                "vc = tokenizer.get_vocab()\n",
                "safe_token_id = vc.get(SAFE_TOKEN)\n",
                "unsafe_token_id = vc.get(UNSAFE_TOKEN)\n",
                "\n",
                "if safe_token_id is None or unsafe_token_id is None:\n",
                "    print(\"Warning: 'safe' or 'unsafe' tokens not found in vocabulary directly.\")\n",
                "    # Fallback or manual check might be needed depending on tokenizer\n",
                "    # For SGuard, they should exist.\n",
                "\n",
                "def classify_jailbreak(prompt: str, threshold: float = 0.6) -> str:\n",
                "    \"\"\"\n",
                "    Determines whether a jailbreak attempt exists in the user prompt provided as an argument and returns the result.\n",
                "\n",
                "    Args:\n",
                "        prompt (str): The user's input prompt.\n",
                "        threshold (float, optinal): Logit threshold value for determining jailbreak.\n",
                "    \n",
                "    Returns:\n",
                "        str: Whether user's input contains jailbreak attempt or not. (unsafe|safe)\n",
                "    \"\"\"\n",
                "    \n",
                "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
                "\n",
                "    # Tokenize and prepare input\n",
                "    # Note: SGuard expects chat template format\n",
                "    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors='pt').to(model.device)\n",
                "\n",
                "    # Generate output\n",
                "    with torch.inference_mode():\n",
                "        output = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=1,\n",
                "            do_sample=False,\n",
                "            return_dict_in_generate=True,\n",
                "            output_logits=True\n",
                "        )\n",
                "\n",
                "    # Decode and classify output\n",
                "    output_score = output.logits[0][0]\n",
                "    selected_logits = torch.tensor([output_score[safe_token_id], output_score[unsafe_token_id]])\n",
                "    probs = torch.softmax(selected_logits, dim=0)\n",
                "\n",
                "    return SAFE_TOKEN if probs[0] >= threshold else UNSAFE_TOKEN\n",
                "\n",
                "print(\"\\n--- Official Inference Verification ---\")\n",
                "\n",
                "# Test with the attacked prompts\n",
                "for res in results:\n",
                "    original_prompt = res['prompt']\n",
                "    suffix = res['suffix']\n",
                "    \n",
                "    # Construct the full attacked prompt\n",
                "    # Note: The suffix is appended to the user prompt\n",
                "    attacked_prompt = original_prompt + \" \" + suffix\n",
                "    \n",
                "    # Classify\n",
                "    verdict = classify_jailbreak(attacked_prompt)\n",
                "    \n",
                "    print(f\"Prompt: {original_prompt[:30]}...\")\n",
                "    print(f\"Verdict: {verdict}\")\n",
                "    print(\"-\" * 20)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}